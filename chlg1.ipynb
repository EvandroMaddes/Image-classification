{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "notebook0e31d29c59.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4UsElV91TrY"
      },
      "source": [
        "#### *In order to run this notebook, you need to first cd into the MaskDataset folder(only if you are using the dataset outside the kaggle competition).* \n",
        "\n",
        "------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXxWDiod0V_0"
      },
      "source": [
        "## This notebook contains two model. One is based on an architecture build by us; the other one is based on the transfer learning approach using the vgg16 architecture and the initial weights for the ImageNet dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QbEWtWy50V_1"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "#in order to hide a not dangerous warning\n",
        "pd.options.mode.chained_assignment = None\n",
        "import os\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.applications import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input #used to preprocess our data for the transfer learning approach\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
        "import json\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMeN-KiZ0V_1"
      },
      "source": [
        "## Pre-processing of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9VYKIA8-0V_1"
      },
      "source": [
        "#set the seed for reproducible experiments\n",
        "SEED = 1234\n",
        "tf.random.set_seed(SEED)  \n",
        "\n",
        "#set the model and the reative hyperparameter to use\n",
        "use_transfer_learning = True \n",
        "#set to use or not data augmentation \n",
        "apply_data_augmentation = True\n",
        "\n",
        "#Create training ImageDataGenerator object. It generate batches of tensor image data with real-time data augmentation.\n",
        "#Preprocess_input function converts images from RGB to BGR, then each color channel is zero-centered with respect to the ImageNet dataset, without scaling.\n",
        "#We choose this function because we apply transfer learning using the weights of ImageNet.\n",
        "#If data augmentation is not selected we only rescale the image\n",
        "if apply_data_augmentation:\n",
        "    train_data_gen = ImageDataGenerator(horizontal_flip=True,\n",
        "                                        height_shift_range= 0.2, \n",
        "                                        width_shift_range=0.2,\n",
        "                                        zoom_range=0.3,\n",
        "                                        rotation_range=20,\n",
        "                                        shear_range=0.15,\n",
        "                                        brightness_range= [0.8, 1],\n",
        "                                        fill_mode='nearest',  \n",
        "                                        preprocessing_function=preprocess_input,\n",
        "                                        )\n",
        "else:\n",
        "    train_data_gen = ImageDataGenerator(rescale=1./255.0)\n",
        "    \n",
        "#take the path of MASKDATASET and store it in dataset_dir\n",
        "cwd = os.getcwd()\n",
        "dataset_dir = cwd\n",
        "\n",
        "#IF YOU ARE ON kaggle competition, USE THIS LINE\n",
        "#dataset_dir = os.path.join(cwd, '../input/artificial-neural-networks-and-deep-learning-2020/MaskDataset')\n",
        "\n",
        "#load the json training file\n",
        "with open(os.path.join(dataset_dir,'train_gt.json')) as f:\n",
        "    dic = json.load(f)\n",
        "\n",
        "#return a DataFrame with the renamed axis labels.\n",
        "df = pd.DataFrame(dic.items())\n",
        "df.rename(columns = {0:'filename', 1:'class'}, inplace = True)\n",
        "\n",
        "#check if the dataset is correctly loaded priting the fist elements and the lenght\n",
        "print(df.head())\n",
        "print('--------')\n",
        "print('total length: ', len(df))\n",
        "\n",
        "#load the images of traing and test folders \n",
        "training_dir = os.path.join(dataset_dir, 'training')\n",
        "test_dir = os.path.join(dataset_dir, 'test')\n",
        "\n",
        "#split training and validation set (75% - 25%) in a stratified fashion, using this as the class labels.\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_set, valid_set = train_test_split(df, test_size=0.25, random_state=SEED, stratify = df['class'])\n",
        "\n",
        "#check if train and valid set have the same proportions of labels; these lines have debug purpose\n",
        "print('% train set label 0: ', round((len(train_set[train_set['class'] == 0])/len(train_set)*100),2))\n",
        "print('% valid set label 0: ', round((len(valid_set[valid_set['class'] == 0])/len(valid_set)*100),2))\n",
        "print('% train set label 1: ', round((train_set[train_set['class'] == 1]['class'].sum()/len(train_set)*100),2))\n",
        "print('% valid set label 1: ', round((valid_set[valid_set['class'] == 1]['class'].sum()/len(valid_set)*100),2))\n",
        "print('% train set label 2: ', round((train_set[train_set['class'] == 2]['class'].sum()/(len(train_set)*2)*100),2))\n",
        "print('% valid set label 2: ', round((valid_set[valid_set['class'] == 2]['class'].sum()/(len(valid_set)*2)*100),2))\n",
        "\n",
        "\n",
        "#batch size, image heigth and width and the total number of classes\n",
        "bs= 16 \n",
        "if use_transfer_learning:\n",
        "    img_h = 400\n",
        "    img_w = 600\n",
        "else:\n",
        "    img_h = 204\n",
        "    img_w = 306\n",
        "    \n",
        "num_classes=3\n",
        "\n",
        "\n",
        "#create the training generator using augmented images. \n",
        "train_set[\"class\"] = train_set[\"class\"].astype(str)\n",
        "train_gen = train_data_gen.flow_from_dataframe(train_set,\n",
        "                                               directory=training_dir,\n",
        "                                               batch_size=bs,\n",
        "                                               x_col='filename',\n",
        "                                               y_col='class',\n",
        "                                               target_size=(img_h, img_w),\n",
        "                                               class_mode='categorical',\n",
        "                                               shuffle=True,\n",
        "                                               seed = SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyqHVn70V_1"
      },
      "source": [
        "## Display of same images in order to understand the applaied data augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6z__12mu0V_1"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "t_x, t_y = next(train_gen)\n",
        "fig, m_axs = plt.subplots(2, 2, figsize = (8, 8))\n",
        "for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n",
        "    c_ax.imshow(c_x[:,:,:])\n",
        "    i = np.where(np.isclose(c_y, 1.0))[0][0]\n",
        "    if i == 0:\n",
        "        c_ax.set_title('noone wearing mask')\n",
        "    elif i == 1:\n",
        "        c_ax.set_title('everyone wearing mask')\n",
        "    else:\n",
        "        c_ax.set_title('someone wearing mask')\n",
        "    c_ax.axis('off')\n",
        "    c_ax.title.set_color('red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SXGA0DyM0V_1"
      },
      "source": [
        "# make basic intensity rescaling on validation set. \n",
        "# If data augmentation is selected then preprocess_input function is applied to the input \n",
        "if apply_data_augmentation:\n",
        "    val_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "else:\n",
        "    val_data_gen = ImageDataGenerator(rescale= 1./255)\n",
        "\n",
        "#VALIDATION\n",
        "valid_set[\"class\"] = valid_set[\"class\"].astype(str)\n",
        "valid_gen = val_data_gen.flow_from_dataframe(dataframe=valid_set, \n",
        "                                           directory=training_dir,\n",
        "                                           batch_size=bs,\n",
        "                                           x_col='filename',\n",
        "                                           y_col='class',\n",
        "                                           target_size=(img_h, img_w),\n",
        "                                           class_mode='categorical',\n",
        "                                           shuffle=True,\n",
        "                                           seed = SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wl16NuDm0V_1"
      },
      "source": [
        "# make basic intensity rescaling on test set. \n",
        "# If data augmentation is selected then preprocess_input function is applaied to the input \n",
        "if apply_data_augmentation:\n",
        "    test_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "else:\n",
        "    test_data_gen = ImageDataGenerator(rescale= 1./255)\n",
        "\n",
        "#TEST\n",
        "test_gen = test_data_gen.flow_from_directory(directory=dataset_dir,\n",
        "                                             batch_size=1,\n",
        "                                             class_mode=None,\n",
        "                                             classes=['test'],\n",
        "                                             target_size=(img_h, img_w),\n",
        "                                             shuffle=False,\n",
        "                                            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsK9w3Cx0V_1"
      },
      "source": [
        "## Create Dataset objects.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "K2_HUEtl0V_1"
      },
      "source": [
        "# Training\n",
        "train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
        "# Repeat\n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "\n",
        "# Validation\n",
        "valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
        "# Repeat\n",
        "valid_dataset = valid_dataset.repeat()\n",
        "\n",
        "\n",
        "# Test\n",
        "test_dataset = tf.data.Dataset.from_generator(lambda: test_gen,\n",
        "                                              output_types=(tf.float32, tf.float32),\n",
        "                                              output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
        "# Repeat\n",
        "test_dataset = test_dataset.repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18sQrVXM0V_1"
      },
      "source": [
        "## Architecture. We build two different architectures: one is based on the transfer learning approach while the other is a custom architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "p1DilkHV0V_1"
      },
      "source": [
        "input_shape = [img_h, img_w, 3] #shape of the model input data\n",
        "\n",
        "if use_transfer_learning:\n",
        "    #Features extraction: we take oly the feature extrator part of the Vgg16 architecture\n",
        "    vgg16 = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
        "    transfer_layer = vgg16.get_layer('block5_pool')\n",
        "    vgg_model = Model(inputs=vgg16.input, outputs=transfer_layer.output)\n",
        "    \n",
        "    #freeze weights up to the ten layer of the vgg16 architecture\n",
        "    for layer in vgg_model.layers[0:10]:\n",
        "        layer.trainable = False\n",
        "    \n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(vgg_model)\n",
        "    \n",
        "    #Classifier is made by two dense layer of 512 neurons for layer. We use also regularization and dropout techniques.\n",
        "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(units=512, activation=tf.keras.activations.relu, kernel_regularizer=tf.keras.regularizers.l1(0.0005))) \n",
        "    model.add(tf.keras.layers.Dense(units=512, activation=tf.keras.activations.relu, kernel_regularizer=tf.keras.regularizers.l1(0.0005))) \n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "else:\n",
        "    #Features extraction composed by 6 repetition of the convolutional block (Conv2D -> Activation -> Pooling)\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Conv2D(32, (3, 3), input_shape = input_shape, activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # Classifier is made by one dense layer of 64 neurons\n",
        "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "#print the model summary for debug purpose \n",
        "if use_transfer_learning:\n",
        "    vgg_model.summary()\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFscc8ZN0V_2"
      },
      "source": [
        "## Optimization parameters: loss, learning rate, validation metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oeP2yiUe0V_2"
      },
      "source": [
        "# Loss\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# learning rate is set wrt the use of transfer learning\n",
        "if use_transfer_learning:\n",
        "    lr = 3e-5\n",
        "else:\n",
        "    lr = 3e-4\n",
        "    \n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "# Validation metrics\n",
        "metrics = ['accuracy']\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4REnB6W0V_2"
      },
      "source": [
        "## Define some *callbacks functions* that are going to be called at the end of each epoch. We used *Early Stopping* in order to stop the training before the model overfits. Moreover we used *ReduceLROnPlateau* that helps the validation loss to decrease and slow down the training part in order to avoid overfitting. We then added another callback called *LearningRateScheduler* in order to tweak the learning rate after each epoch given a function `scheduler` ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-W2c4gXY0V_2"
      },
      "source": [
        "class CustomEarlyStopping(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, patience=0):\n",
        "        super(CustomEarlyStopping, self).__init__()\n",
        "        self.patience = patience\n",
        "        # best_weights to store the weights at which the minimum loss occurs.\n",
        "        self.best_weights = None\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        # The number of epoch it has waited when loss is no longer minimum.\n",
        "        self.wait = 0\n",
        "        # The epoch the training stops at.\n",
        "        self.stopped_epoch = 0\n",
        "        # Initialize the best as infinity.\n",
        "        self.best = np.Inf\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        accuracy = logs.get(\"accuracy\")\n",
        "        val_accuracy = logs.get(\"val_accuracy\")\n",
        "        if epoch > 10 and np.less(val_accuracy + 5, accuracy):\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                self.model.stop_training = True\n",
        "                print(\"Restoring model weights from the end of the best epoch.\")\n",
        "                self.model.set_weights(self.best_weights)\n",
        "        else:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            # Record the best weights if current results is better (less).\n",
        "            self.best_weights = self.model.get_weights()\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.stopped_epoch > 0:\n",
        "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mchfmyru0V_2"
      },
      "source": [
        "callbacks = []\n",
        "# --------------\n",
        "early_stop = True\n",
        "attenuate_lr = True\n",
        "schedule_lr = False\n",
        "use_custom = False #set the custom early stopping previously defined\n",
        "\n",
        "#early stopping is set with a patience of four epoch on the reduction (delta of 0.005) of the validation accuracy\n",
        "if early_stop:\n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                          min_delta=0.005,\n",
        "                                          patience = 4,\n",
        "                                          verbose=1,\n",
        "                                          mode='min',                                        \n",
        "                                         )\n",
        "    callbacks.append(es)\n",
        "\n",
        "# Attenuate learning rate is set with a patience of three on the reduction of the validation accuracy.\n",
        "#the new_lr is equal to the previous multiply by 0.3. this is repeated up to a minimum learninig rate of 10^(-7)\n",
        "if attenuate_lr:\n",
        "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
        "                                                     mode='min', \n",
        "                                                     factor=0.3, \n",
        "                                                     patience=3, \n",
        "                                                     min_lr=1e-7, \n",
        "                                                     verbose=1, \n",
        "                                                     cooldown=0)\n",
        "    callbacks.append(reduce_lr)\n",
        "\n",
        "# This function keeps the initial learning rate for the first ten epochs and decreases it exponentially after that.  \n",
        "def scheduler(epoch, lr):\n",
        "    k = 0.1\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-k)\n",
        "\n",
        "if schedule_lr:\n",
        "    cb = tf.keras.callbacks.LearningRateScheduler(schedule=scheduler, verbose=1)\n",
        "    callbacks.append(cb)\n",
        "    \n",
        "#save checkpoint of the best model wrt the validation loss\n",
        "callbacks.append(tf.keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True))\n",
        "\n",
        "#set to use the ustom early stopping\n",
        "if use_custom:\n",
        "    callbacks.append(CustomEarlyStopping())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm3EWbuR0V_2"
      },
      "source": [
        "## Now we are ready to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zPTtNy990V_2"
      },
      "source": [
        "#print start time of the model's training\n",
        "from datetime import datetime\n",
        "start = datetime.now()\n",
        "print(start)\n",
        "\n",
        "#fit of the model. It runs for a maximum of ten epochs. each epoch has a lenght of training set divided by batch size\n",
        "history=model.fit(x=train_dataset,\n",
        "          epochs=100,  \n",
        "          steps_per_epoch=len(train_gen),\n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=len(valid_gen),\n",
        "          callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "PlEzyl6r0V_2"
      },
      "source": [
        "#print the total time of training for debug purpose\n",
        "end = datetime.now()\n",
        "delta = str(end-start)\n",
        "print(\"============================================\")\n",
        "print(\"Time taken (h/m/s): %s\" %delta[:7])\n",
        "print(\"============================================\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr8Se-rC0V_2"
      },
      "source": [
        "## Import the best checkpoint of the model for all the epochs we went through. We decided that the best one is the one with the least validation  loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "iOLzjGHJ0V_2"
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('best_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8LfonTU0V_2"
      },
      "source": [
        "## Create a function to export predictions to .csv format (required by kaggle to test our model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SjHWFaCI0V_2"
      },
      "source": [
        "def create_csv(results, results_dir='./'):\n",
        "\n",
        "    csv_fname = 'results_'\n",
        "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "\n",
        "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
        "\n",
        "        f.write('Id,Category\\n')\n",
        "\n",
        "        for key, value in results.items():\n",
        "            f.write(key + ',' + str(value) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoR73Ww60V_2"
      },
      "source": [
        "# Make Predictions with our new trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ObrHu38j0V_2"
      },
      "source": [
        "test_gen.reset()\n",
        "predictions = model.predict_generator(test_gen, len(test_gen), verbose=1)\n",
        "\n",
        "results = {}\n",
        "import ntpath\n",
        "images = test_gen.filenames\n",
        "\n",
        "i = 0\n",
        "\n",
        "for p in predictions:\n",
        "  prediction = np.argmax(p)\n",
        "  image_name = ntpath.basename(images[i])\n",
        "  results[image_name] = str(prediction)\n",
        "  i = i + 1\n",
        "    \n",
        "create_csv(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OweJwEcG0V_2"
      },
      "source": [
        "## Plot accuracy and loss for both training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Qywc3_TP0V_2"
      },
      "source": [
        "# plot of the training and validation loss\n",
        "with plt.rc_context({'axes.edgecolor':'orange', 'xtick.color':'red', 'ytick.color':'green', 'figure.facecolor':'white'}):\n",
        "    plt.plot(history.history['loss'],'r',label='training loss')\n",
        "    plt.plot(history.history['val_loss'],label='validation loss')\n",
        "    x=plt.xlabel('# epochs')\n",
        "    y=plt.ylabel('loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QoHftkvV0V_2"
      },
      "source": [
        "# plot of the training and validation accuracy\n",
        "with plt.rc_context({'axes.edgecolor':'orange', 'xtick.color':'red', 'ytick.color':'green', 'figure.facecolor':'white'}):\n",
        "    plt.plot(history.history['accuracy'],'r',label='training accuracy')\n",
        "    plt.plot(history.history['val_accuracy'],label='validation accuracy')\n",
        "    x=plt.xlabel('# epochs')\n",
        "    y=plt.ylabel('loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}