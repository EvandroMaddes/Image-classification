# REPORT: IMAGE CLASSIFICATION [COMPETITION](https://www.kaggle.com/c/artificial-neural-networks-and-deep-learning-2020)  
Firstly, we applied some preprocessing: since the images are 612x408, 3 channels, our first choice was to reduce the input shape, halving the images and applying rescale. For the batch size we started with 8 but then we increased it in order to add complexity to our model, reaching at the end a bs of 16. We then applied some basic data augmentation: horizontal flip (not vertical, because that would not be a real scenario image), weight and height shift, zoom, rotation and brightness, aiming to overcome overfitting. We split the training data using 75% for training the model and 25% to validate it. We used the function stratify in order to maintain the same proportions of labels between the training and the validation set. We added a check on the % of labels in each set (33% for each of them). After, we plotted the images to see them with their corresponding labels and we created the generators for each set, not augmenting the validation set. Note that we set the random seed to a fixed number in order to reproduce the training at each trial.  
After the preprocessing step, we started to build our model, without transfer learning at first, for evaluating the performances that could be obtained. The performances reached by the convolutional base seen at lesson was not satisfactory, so we changed the architecture using a set of Conv2D, relu, MaxPooling2D layers repeated six times. About the fully connected neurons part, we decided to use just a dense layer of 64 neurons and some dropout to reduce overfitting. This model reached about 0.8 score on the test accuracy that is not that high.  
We decided to exploit a transfer learning approach in order to see the real improvements and make a comparison. We used the famous Vgg16 architecture, one of the most used models even if it is from 2014. In order to use this architecture, we added a step in the preprocess phase to normalize images in input with the average and standard deviation of the ImageNet dataset images (using the *preprocess_input function* given by Keras). Vgg16 requires by default images at 224x224x3 so we put our input to that size. We used the ImageNet weights, without including the multi-layer perceptron part, because we needed our model to be trained on our specific purpose and outputs. At the beginning we froze all the convolutional part (the 18 layers of the vgg) but then the validation accuracy did not improve that much, so we started to do some fine tuning, gradually decreasing the number of froze layers up to the tenth layer. Moreover, we changed the input size of the images to their original size (612x408x3) because we found out that by shrinking the images, we lost some relevant information. For what concerns the fully connected part, we tried a lot of configurations in order to have a good tradeoff between not generalizing too much and to actually learn from data, so we added dropout after the flatten layer and regularization on both dense layer of 512 neurons, reaching a good validation accuracy while still learning from data.  
At the end, the final configuration of our best model is composed of 2 dense layers of 512 neurons, a dropout of 0.5 after the flatten layer with a relu activation function in order not to have our gradient to vanish. Softmax at the output because we have a problem of multi-classification. For the loss function we kept the standard CategoricalCrossentropy and we used the Adam optimizer, that have some good features like momentum integrated in it. The learning rate for our best model is in the order of 10<sup>-5</sup> even if we started with a power of 10<sup>-4</sup>, that at the end we used only for the model without transfer learning. We set some callbacks useful to fight the overfitting like the EarlyStopping or the ReduceLROnPlateau, or also the scheduler of learning rate even if at the end we did not included it in the final model. We decided to keep the best model after each epoch based on the smallest validation loss achieved. We set the number of epochs to be high, because we rely on early stopping to finish before reaching the last epoch. 
